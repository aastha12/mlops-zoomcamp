{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4.1 Model Deployment\n",
    "\n",
    "### Recap:\n",
    "\n",
    "We looked at the \"Design\" phase where we gather the requirements and understand if machine learning is the right solution for our problem. Then we looked at the \"Training\" phase where we talked about experiment tracking like how we and we also talked about productionizing our jupyter notebook and turning this into a machine learning pipeline. \n",
    "\n",
    "Now that the model is registered in MLflow Model Registry and production ready, we need to deploy that so that we can get the prediction result for the given data to realize its value.\n",
    "\n",
    "### Model Deployment:\n",
    "\n",
    "There are primarily two kinds of deployments:\n",
    "- Batch (offline) - runs regularly\n",
    "- Online - Up & running all the time with two sub-options:\n",
    "    - Web service\n",
    "    - Streaming\n",
    "\n",
    "<img src=\"notes-images/deployment-types.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Deployment:\n",
    "\n",
    "The model doesn't run all the time, but we regularly apply it to new data. The regularity could be hours, days, week *etc*.\n",
    "A typical batch deployment pipeline is sketched below. Let's say we have a database with all the data.\n",
    "The scoring job fetches some data from the database and applies the model to that data. The predictions are then written to another\n",
    "database. Another software can read predictions and react to them, for example, by preparing a report or raising an alarm.\n",
    "\n",
    "<img src=\"notes-images/batch-deployment-pipeline.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web service deployment:\n",
    "\n",
    "Users use an app that communicates with the backend, which in turn communicates with a service that runs the model.\n",
    "The model needs to be always running. When the client sends a request,\n",
    "it initiates the connection to the backend service which remains open during the processing operation.\n",
    "\n",
    "<img src=\"notes-images/web-deployment-pipeline.png\" width=\"700\"/>\n",
    "\n",
    "For example, a client wants to know the duration of an upcoming taxi ride. The mobile app sends a request to the backend, which performs calculations using\n",
    "the model in the service and returns the result back to the app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming deployment:\n",
    "\n",
    "In the streaming settings, we have producers (they produce events) and consumers (they consume events). Producers pass some events to the event stream. \n",
    "Consumers read it and respond to events. Here we have one-to-many (single producer) or many-to-many (multiple producers) client-server relationship.\n",
    "\n",
    "Producer send an event but does not really care what happens with it since there is not explicit connection between producers and consumers.\n",
    "\n",
    "<img src=\"notes-images/streaming-deployment-pipeline.png\" width=\"700\"/>\n",
    "\n",
    "Let's have a look at the iweb service example. Now the backend becomes a producer. It generates the event \"Ride started\" containing all information about\n",
    "the ride. Then multiple services consume this stream and run something for it:  \n",
    "\n",
    "- consumer 1: tip prediction\n",
    "- consumer 2: duration prediction consumer\n",
    "- consumer 3: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4.2  Web-services: Deploying models with Flask and Docker\n",
    "\n",
    "To deploy as a web service we go through steps:\n",
    "1. Getting the Python envrionment used to train/test the model using pipenv \n",
    "2. Re-writing the prediction script and wrap it with a backend (Flask used here) \n",
    "3. Creating a Docker Container and putting our prediction backend with it along with the Python environment\n",
    "\n",
    "#### Python Environment:\n",
    "\n",
    "We want to use the model developed in week 1 of the course. For that, we'll need to obtain the python environment we used to train and test the model for consistency. To obtain the packages and the package versions of the current python environment (even if conda), we use pip freeze; This outputs the installed packages and their versions. In our case, we're mostly interested in getting the scikit-learn version. So we grep scikit to only get lines with scikit in them:\n",
    "\n",
    "```\n",
    "cd \"mlops-zoomcamp/04-deployment/web-service\" (go to the web-service folder)\n",
    "```\n",
    "\n",
    "```\n",
    "pip freeze | grep scikit-learn \n",
    " OR \n",
    "pip list | grep scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Pipfile or Pipfile.lock files already exist, make sure to delete them before creating a new virtual environment. When you run the below commands, it will create a new Pipfile and a Pipfile.lock stores the versions of the packages that we want.\n",
    "\n",
    "With pipenv, create a new virtual environment:\n",
    "\n",
    "```\n",
    "pipenv install scikit-learn==1.2.2 flask\n",
    "```\n",
    "\n",
    "Activate the environment using:\n",
    "```\n",
    "pipenv shell\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the prediction script:\n",
    "\n",
    "Quick recap: Our week 1 model writes 2 pickle files. One is the Linear Regressor, the other is the DictVectorizer object. The prediction moves through 3 steps:\n",
    "\n",
    "1- Feature Engineering 2- DictVectorizer 3- Regressor\n",
    "\n",
    "As a web service, our predictor will take a dictionary of a single \"row\" rather than a Pandas DataFrame as input.\n",
    "\n",
    "Two functions to deal with JSON files:\n",
    "- `jsonify(D)` transforms a dictionary D into a JSON\n",
    "- `request.get_json()` reads the JSON passed to the app\n",
    "\n",
    "Check out the predict.py file to see how we added Flask to the prediction script. Make sure you are in the right virtual environment and run `python predict.py` to run the prediction script.\n",
    "\n",
    "To request a prediction from the server, we create another file test.py. This file will post its ride information to the server and print out the response (i.e: The predicted duration). While the prediction script is running on the terminal, open up another terminal and make sure you are in the right virtual environment and run `python test.py` to run the get a prediction. You will get this output: `{'duration': 26.43883355119793}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying as WGSI:\n",
    "\n",
    "When you run `pythong predict.py` you'll notice this warning message:\n",
    "```\n",
    " * Environment: production\n",
    "   WARNING: This is a development server. Do not use it in a production deployment.\n",
    "   Use a production WSGI server instead.\n",
    "```\n",
    "\n",
    "Our current flask set up is for the development environment. Install gunicorn in order to solve the following production environment type warning. gunicorn is one of the production servers.\n",
    "\n",
    "To deploy the model into production, we use gunicorn to deploy the web service: \n",
    "\n",
    "`pipenv install gunicorn` (you'll notice that the Pipfile and Pipfile.lock files also got updated with gunicorn)\n",
    "\n",
    "`gunicorn --bind=0.0.0.0:9696 predict:app`\n",
    "\n",
    "where predict is the predict.py located in the current directory, and app is the Flask app defined on that file (See above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Container:\n",
    "\n",
    "Now we want to deploy our predictor into a Docker Container for reproducibility, scalability, security.\n",
    "\n",
    "Check out the version of python by typing `python -V` in the command line:\n",
    "```\n",
    "(web-service) aasth@Aasthas-Air web-service % python -V\n",
    "Python 3.9.6\n",
    "```\n",
    "\n",
    "In our case, the python version is 3.9.6.\n",
    "\n",
    "In our DockerFile (refer to Dockerfile):\n",
    "\n",
    "- we will write `FROM python:3.9-slim`. \"-slim\" is just a reduced size of the image.\n",
    "\n",
    "- If we type just `RUN pipenv install` then a virtual environment will be created inside of docker which is not needed as docker is an isolated container as it is. So we can install the packges directly on the system by running `RUN pipenv install --system`. The `--deploy` makes sure Pipfile.lock is up-to-date and will crash if it isn't. So final command is `RUN pipenv install --system --deploy`\n",
    "\n",
    "- The lines `COPY [ \"predict.py\", \"lin_reg.bin\", \"./\" ]` and `COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]` means copy the predict.py, lin_reg.bin, Pipfile and Pipfile.lock into the current directory.\n",
    "\n",
    "- We specify the current directory by `WORKDIR /app`. This command creates and cd's (enters) into the \"/app\" directory\n",
    "\n",
    "\n",
    "Open the Docker app and while it is open in the backgroun, we then build the Docker Image with:\n",
    "\n",
    "`docker build -t ride-duration-prediction-service:v1 .`\n",
    "\n",
    "In the above, `ride-duration-prediction-service` is the image name and `v1` is the tag.\n",
    "\n",
    "And run the container that was built with:\n",
    "\n",
    "`docker run -it --rm -p 9696:9696 ride-duration-prediction-service:v1`\n",
    "\n",
    "Now when we request predictions like earlier, we're instead calling the WGSI within the Docker Container. While the container is running in the terminal , open another terminal and make sure you are in the right virtual environment and run `python test.py` to run the get a prediction. You will get this output: `{'duration': 26.43883355119793}`.\n",
    "\n",
    "So far we have packaged the model in a docker file that can run in every docker compatible compute to serve. However the model we used was directly loaded from the local path where it was stored and we had learnt in previous sessions that the candidate models were stored in model registry that we were supposed to use. Hence, in the next section we will learn how to fetch the model from model registry to serve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-zoomcamp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
