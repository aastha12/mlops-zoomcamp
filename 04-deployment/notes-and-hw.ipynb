{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4.1 Model Deployment\n",
    "\n",
    "### Recap:\n",
    "\n",
    "We looked at the \"Design\" phase where we gather the requirements and understand if machine learning is the right solution for our problem. Then we looked at the \"Training\" phase where we talked about experiment tracking like how we and we also talked about productionizing our jupyter notebook and turning this into a machine learning pipeline. \n",
    "\n",
    "Now that the model is registered in MLflow Model Registry and production ready, we need to deploy that so that we can get the prediction result for the given data to realize its value.\n",
    "\n",
    "### Model Deployment:\n",
    "\n",
    "There are primarily two kinds of deployments:\n",
    "- Batch (offline) - runs regularly\n",
    "- Online - Up & running all the time with two sub-options:\n",
    "    - Web service\n",
    "    - Streaming\n",
    "\n",
    "<img src=\"notes-images/deployment-types.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Deployment:\n",
    "\n",
    "The model doesn't run all the time, but we regularly apply it to new data. The regularity could be hours, days, week *etc*.\n",
    "A typical batch deployment pipeline is sketched below. Let's say we have a database with all the data.\n",
    "The scoring job fetches some data from the database and applies the model to that data. The predictions are then written to another\n",
    "database. Another software can read predictions and react to them, for example, by preparing a report or raising an alarm.\n",
    "\n",
    "<img src=\"notes-images/batch-deployment-pipeline.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web service deployment:\n",
    "\n",
    "Users use an app that communicates with the backend, which in turn communicates with a service that runs the model.\n",
    "The model needs to be always running. When the client sends a request,\n",
    "it initiates the connection to the backend service which remains open during the processing operation.\n",
    "\n",
    "<img src=\"notes-images/web-deployment-pipeline.png\" width=\"700\"/>\n",
    "\n",
    "For example, a client wants to know the duration of an upcoming taxi ride. The mobile app sends a request to the backend, which performs calculations using\n",
    "the model in the service and returns the result back to the app.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming deployment:\n",
    "\n",
    "In the streaming settings, we have producers (they produce events) and consumers (they consume events). Producers pass some events to the event stream. \n",
    "Consumers read it and respond to events. Here we have one-to-many (single producer) or many-to-many (multiple producers) client-server relationship.\n",
    "\n",
    "Producer send an event but does not really care what happens with it since there is not explicit connection between producers and consumers.\n",
    "\n",
    "<img src=\"notes-images/streaming-deployment-pipeline.png\" width=\"700\"/>\n",
    "\n",
    "Let's have a look at the iweb service example. Now the backend becomes a producer. It generates the event \"Ride started\" containing all information about\n",
    "the ride. Then multiple services consume this stream and run something for it:  \n",
    "\n",
    "- consumer 1: tip prediction\n",
    "- consumer 2: duration prediction consumer\n",
    "- consumer 3: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4.2  Web-services: Deploying models with Flask and Docker\n",
    "\n",
    "`Reference: https://github.com/ayoub-berdeddouch/mlops-journey/blob/main/deployment-04.md`\n",
    "\n",
    "To deploy as a web service we go through steps:\n",
    "1. Getting the Python envrionment used to train/test the model using pipenv \n",
    "2. Re-writing the prediction script and wrap it with a backend (Flask used here) \n",
    "3. Creating a Docker Container and putting our prediction backend with it along with the Python environment\n",
    "\n",
    "#### Python Environment:\n",
    "\n",
    "We want to use the model developed in week 1 of the course. For that, we'll need to obtain the python environment we used to train and test the model for consistency. To obtain the packages and the package versions of the current python environment (even if conda), we use pip freeze; This outputs the installed packages and their versions. In our case, we're mostly interested in getting the scikit-learn version. So we grep scikit to only get lines with scikit in them:\n",
    "\n",
    "```\n",
    "cd \"mlops-zoomcamp/04-deployment/web-service\" (go to the web-service folder)\n",
    "```\n",
    "\n",
    "```\n",
    "pip freeze | grep scikit-learn \n",
    " OR \n",
    "pip list | grep scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Pipfile or Pipfile.lock files already exist, make sure to delete them before creating a new virtual environment. When you run the below commands, it will create a new Pipfile and a Pipfile.lock stores the versions of the packages that we want.\n",
    "\n",
    "Before running the below command, make sure you are in any active virtual environment (for example, you can use the `mlops-zoomcamp-venv` by typing `conda activate /opt/homebrew/anaconda3/envs/mlops-zoomcamp-venv`). Then once you are in an active environment (in our case `mlops-zoomcamp-venv`), use pipenv to create a new virtual environment:\n",
    "\n",
    "```\n",
    "pipenv install scikit-learn==1.2.2 flask\n",
    "```\n",
    "\n",
    "Activate the environment using:\n",
    "```\n",
    "pipenv shell\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the prediction script:\n",
    "\n",
    "Quick recap: Our week 1 model writes 2 pickle files. One is the Linear Regressor, the other is the DictVectorizer object. The prediction moves through 3 steps:\n",
    "\n",
    "1- Feature Engineering 2- DictVectorizer 3- Regressor\n",
    "\n",
    "As a web service, our predictor will take a dictionary of a single \"row\" rather than a Pandas DataFrame as input.\n",
    "\n",
    "Two functions to deal with JSON files:\n",
    "- `jsonify(D)` transforms a dictionary D into a JSON\n",
    "- `request.get_json()` reads the JSON passed to the app\n",
    "\n",
    "Check out the predict.py file to see how we added Flask to the prediction script. Make sure you are in the right virtual environment and run `python predict.py` to run the prediction script.\n",
    "\n",
    "To request a prediction from the server, we create another file test.py. This file will post its ride information to the server and print out the response (i.e: The predicted duration). While the prediction script is running on the terminal, open up another terminal and make sure you are in the right virtual environment and run `python test.py` to run the get a prediction. You will get this output: `{'duration': 26.43883355119793}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying as WGSI:\n",
    "\n",
    "When you run `pythong predict.py` you'll notice this warning message:\n",
    "```\n",
    " * Environment: production\n",
    "   WARNING: This is a development server. Do not use it in a production deployment.\n",
    "   Use a production WSGI server instead.\n",
    "```\n",
    "\n",
    "Our current flask set up is for the development environment. Install gunicorn in order to solve the following production environment type warning. gunicorn is one of the production servers.\n",
    "\n",
    "To deploy the model into production, we use gunicorn to deploy the web service: \n",
    "\n",
    "`pipenv install gunicorn` (you'll notice that the Pipfile and Pipfile.lock files also got updated with gunicorn)\n",
    "\n",
    "`gunicorn --bind=0.0.0.0:9696 predict:app`\n",
    "\n",
    "where predict is the predict.py located in the current directory, and app is the Flask app defined on that file (See above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Container:\n",
    "\n",
    "Now we want to deploy our predictor into a Docker Container for reproducibility, scalability, security.\n",
    "\n",
    "Check out the version of python by typing `python -V` in the command line:\n",
    "```\n",
    "(web-service) aasth@Aasthas-Air web-service % python -V\n",
    "Python 3.9.6\n",
    "```\n",
    "\n",
    "In our case, the python version is 3.9.6.\n",
    "\n",
    "In our DockerFile (refer to Dockerfile):\n",
    "\n",
    "- we will write `FROM python:3.9-slim`. \"-slim\" is just a reduced size of the image.\n",
    "\n",
    "- If we type just `RUN pipenv install` then a virtual environment will be created inside of docker which is not needed as docker is an isolated container as it is. So we can install the packges directly on the system by running `RUN pipenv install --system`. The `--deploy` makes sure Pipfile.lock is up-to-date and will crash if it isn't. So final command is `RUN pipenv install --system --deploy`\n",
    "\n",
    "- The lines `COPY [ \"predict.py\", \"lin_reg.bin\", \"./\" ]` and `COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]` means copy the predict.py, lin_reg.bin, Pipfile and Pipfile.lock into the current directory.\n",
    "\n",
    "- We specify the current directory by `WORKDIR /app`. This command creates and cd's (enters) into the \"/app\" directory\n",
    "\n",
    "\n",
    "Open the Docker app and while it is open in the backgroun, we then build the Docker Image with:\n",
    "\n",
    "`docker build -t ride-duration-prediction-service:v1 .`\n",
    "\n",
    "In the above, `ride-duration-prediction-service` is the image name and `v1` is the tag.\n",
    "\n",
    "And run the container that was built with:\n",
    "\n",
    "`docker run -it --rm -p 9696:9696 ride-duration-prediction-service:v1`\n",
    "\n",
    "Now when we request predictions like earlier, we're instead calling the WGSI within the Docker Container. While the container is running in the terminal , open another terminal and make sure you are in the right virtual environment and run `python test.py` to run the get a prediction. You will get this output: `{'duration': 26.43883355119793}`.\n",
    "\n",
    "So far we have packaged the model in a docker file that can run in every docker compatible compute to serve. However the model we used was directly loaded from the local path where it was stored and we had learnt in previous sessions that the candidate models were stored in model registry that we were supposed to use. Hence, in the next section we will learn how to fetch the model from model registry to serve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4.3  Web-services: Getting the models from the model registry (MLflow)\n",
    "\n",
    "`Reference: https://github.com/BPrasad123/MLOps_Zoomcamp/tree/main/Week4`\n",
    "\n",
    "This time we are going to run a fresh experiment to train a new model (Random Forest) on the same dataset and register the model in MLflow Model Registry.\n",
    "\n",
    "We can do this by providing a RUN ID manually (like we are doing in this case) or by using a s3 link as the tracking uri (which is not shown in this segment but you can refer to Alexey's video for it.)\n",
    "\n",
    "#### Python envrionment:\n",
    "\n",
    "Go to the correct directory:\n",
    "\n",
    "```\n",
    "cd \"mlops-zoomcamp/04-deployment/web-service-mlflow\" (go to the web-service-mlflow folder)\n",
    "```\n",
    "\n",
    "and activate any virtual environment (let's say mlops-zoomcamp-venv) by doing `source /opt/homebrew/anaconda3/bin/activate ml\n",
    "ops-zoomcamp-venv`). Then once you are in an active environment (in our case `mlops-zoomcamp-venv`), use pipenv to create a new virtual environment:\n",
    "\n",
    "```\n",
    "pipenv install mlflow\n",
    "```\n",
    "\n",
    "The above command will first create a new virtual environment and then install the mlflow package and all the packages in the Pipfile.\n",
    "\n",
    "Activate the environment using:\n",
    "```\n",
    "pipenv shell\n",
    "```\n",
    "\n",
    "#### Running the jupyter notebook\n",
    "\n",
    "Once you are in the `web-service-mlflow` folder, then run `mlflow ui`:\n",
    "\n",
    "```\n",
    "(web-service-mlflow) aasth@Aasthas-MacBook-Air web-service-mlflow % mlflow ui                                        \n",
    "[2024-01-05 21:04:17 -0800] [62488] [INFO] Starting gunicorn 20.1.0\n",
    "[2024-01-05 21:04:17 -0800] [62488] [INFO] Listening at: http://127.0.0.1:5000 (62488)\n",
    "```\n",
    "\n",
    "Click on `http://127.0.0.1:5000` link to open up the MLFlow tracking page and then run the random-forest.ipynb jupyter notebook.\n",
    "\n",
    "<img src=\"notes-images/webservice-mlflow-rf-jupyter.png\" width=\"700\"/>\n",
    "\n",
    "Remember to update the RUN_ID in the jupyter notebook based on the run_id of the mlflow experiment that you want to finalize. For example, in the below screenshot you can see the run id is 229c53c2f7b349eca20d7c482efb4bc8 so that's what we entered in the jupyter notebook:\n",
    "\n",
    "<img src=\"notes-images/webservice-mlflow-rf-ui.png\" width=\"700\"/>\n",
    "\n",
    "There are multiple ways to use the logged model. If we are using runs/RUN_ID/model (the method we used in the screenshot) then we run with risk of availability cause the tracking server can go down. However, if fetch the artifact directly from S3 then we are not dependent on the artifact server. Please check the predict.py script to see the commented line on how you can use S3.\n",
    "\n",
    "In another terminal run test.py to see if we are getting the predicted result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Module 4.4 was optional so I skipped it for now.*\n",
    "\n",
    "## Module 4.5   Batch: Preparing a scoring script\n",
    "\n",
    "Typical approach for deploying a model in batch model:\n",
    "\n",
    "- Create a notebook/training script to train a model and save it\n",
    "- Create a notebook to load the trained model and make prediction on the new data\n",
    "- Convert the notebook to an inference script\n",
    "- Clean and parameterize the script\n",
    "- Schedule the inference script if required\n",
    "\n",
    "#### Python envrionment:\n",
    "\n",
    "Go to the correct directory:\n",
    "\n",
    "```\n",
    "cd \"mlops-zoomcamp/04-deployment/batch\" (go to the batch folder)\n",
    "```\n",
    "\n",
    "and activate any virtual environment (let's say mlops-zoomcamp-venv) by doing `source /opt/homebrew/anaconda3/bin/activate ml\n",
    "ops-zoomcamp-venv`). Then once you are in an active environment (in our case `mlops-zoomcamp-venv`), use pipenv to create a new virtual environment:\n",
    "\n",
    "```\n",
    "pipenv install mlflow\n",
    "```\n",
    "\n",
    "The above command will first create a new virtual environment and then install the mlflow package and all the packages in the Pipfile.\n",
    "\n",
    "Activate the environment using:\n",
    "```\n",
    "pipenv shell\n",
    "```\n",
    "\n",
    "#### Running the jupyter notebook\n",
    "\n",
    "Once you are in the `batch` folder, then run `mlflow ui`:\n",
    "\n",
    "```\n",
    "(batch) aasth@Aasthas-MacBook-Air batch % mlflow ui\n",
    "[2024-01-07 17:06:57 -0800] [67271] [INFO] Starting gunicorn 20.1.0\n",
    "[2024-01-07 17:06:57 -0800] [67271] [INFO] Listening at: http://127.0.0.1:5000 (67271)\n",
    "```\n",
    "\n",
    "Click on `http://127.0.0.1:5000` link to open up the MLFlow tracking page and then run the random-forest.ipynb jupyter notebook.\n",
    "\n",
    "<img src=\"notes-images/webservice-mlflow-rf-jupyter.png\" width=\"700\"/>\n",
    "\n",
    "You'll notice a mlruns folder is created when you run the jupyter notebook. The mlruns folder has a default folder called 0 and a new folder that consists of information about your current experiment run.\n",
    "\n",
    "### Score notebook and script\n",
    "\n",
    "Now, open the score.ipynb. In this notebook, we will load our model from mlflow (which is why it is important to run the random-forest.ipynb first) and then make a prediction on some data.\n",
    "\n",
    "The score.py is just a python script version of the score.ipynb notebook. Open a new terminal (while having `mlflow ui` run in another terminal) and check if the score.py file works fine.\n",
    "\n",
    "```\n",
    "(batch) aasth@Aasthas-MacBook-Air batch % python score.py green 2022 3\n",
    "```\n",
    "\n",
    "You should see a `2022-02.parquet` file in the output/green folder.\n",
    "\n",
    "In the course, the model is loaded via a S3 link but I am loading the pickled version of the model (as I haven't used S3) so that it works when I deploy it using prefect.\n",
    "\n",
    "### Prefect deployment for score.py \n",
    "\n",
    "*Note: If you face issues with the package dependencies, maybe try removing the virtual environment completely using `pipenv --rm` and then (while you are in another virtual env like `mlops-zoomcamp-venv`) then run `pipenv install`.*\n",
    "\n",
    "1. Open the Prefect UI using `prefect server start` to spin up a localhost instance. When you start the server, you'll see a message saying:\n",
    "```\n",
    "Configure Prefect to communicate with the server with:\n",
    "\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "```\n",
    "\n",
    "Open up another terminal with the `batch` environment activated and run the command `prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api`\n",
    "\n",
    "2. Run `prefect project init` to initialize the prefect project in the batch directory and to create the necessary prefect files like:\n",
    "  - .prefectignore\n",
    "  - deployment.yaml: usefule for templating if you're making multiple deployments from the same project\n",
    "  - prefect.yaml\n",
    "  - .prefect/: this is a hidden folder\n",
    "\n",
    "\n",
    "3. Run `prefect deploy \"/Users/aasth/Desktop/Data analytics/MLOps/datatalks-zoomcamp/mlops-zoomcamp/04-deployment/batch\"/score.py:ride_duration_prediction -n my-first-deployment -p local-workpool`. You might need to create a worker called local-workpool first. In that case, run command (4) then terminate it and run command (3).\n",
    "\n",
    "4. Then you can run `prefect worker start -p local-workpool -t process` and it will create the work pool called local-workpool if it didn't already exist.\n",
    "\n",
    "5. Then run the `my-first-deployment` deployment and it should be successful.\n",
    "\n",
    "### Prefect deploymnet for score_backfill.py:\n",
    "\n",
    "Similar till step (2) of \"Prefect deployment for score.py\" except the following steps will be: \n",
    "\n",
    "3. Run `prefect deploy \"/Users/aasth/Desktop/Data analytics/MLOps/datatalks-zoomcamp/mlops-zoomcamp/04-deployment/batch\"/score_backfill.py:ride_duration_prediction_backfill -n backfill-deployment -p local-workpool`\n",
    "\n",
    "4. Then you can run `prefect worker start -p local-workpool -t process` and it will create the work pool called local-workpool if it didn't already exist.\n",
    "\n",
    "5. Then run the `backfill-deployment` deployment and it should be successful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-zoomcamp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
