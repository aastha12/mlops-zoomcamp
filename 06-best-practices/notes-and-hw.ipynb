{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6.1 : Testing Python code with pytest\n",
    "\n",
    "In this section, we will work on the [streaming code in the 04-deployment section](/mlops-zoomcamp/04-deployment/streaming/lambda_function.py). We will make the code better from an engineering pov by adding tests.\n",
    "\n",
    "\n",
    "### Creating and activate virtual environment:\n",
    "\n",
    "1. Go to the correct directory by: \n",
    "```bash \n",
    "cd mlops-zoomcamp/06-best-practices/code\n",
    "```\n",
    "2. Activate any existing conda virtual evironment: \n",
    "```bash\n",
    ". /opt/homebrew/anaconda3/bin/activate && conda activate /opt/homebrew/anaconda3/envs/mlops-zoomcamp-venv;\n",
    "```\n",
    "3. Create a new virtual environment: `pipenv install`\n",
    "4. Activate it by `pipenv shell`\n",
    "5. Install pytest with `pipenv install --dev pytest`. We use the dev argument cause we want pytest only in the dev environment and not in the production environment.\n",
    "6. Find the location of your virtual environment by typing `pipenv --venv`. You'll get the path `/Users/aasth/.local/share/virtualenvs/code-JCzC6QQn` to the venv. Copy the path.\n",
    "7. We need to set up our python envionment in VSCode. Hit `Cmd+Shift+P` -> `Select Python Interpreter` and paste the path of the venv that you copied in step 6.\n",
    "8. We will configure the python tests. Click on the `Testing` tab which is lcoated on the left panel of VSCode. Click om the `Configure Python Tests` button. Select `pytest` and the `test` directory.\n",
    "\n",
    "### Testing if Docker works\n",
    "\n",
    "1. Open your Docker app and in the terminal with the `code` environment activated, run `docker build -t stream-model-duration:v2 .`.\n",
    "\n",
    "*Note: If you already have a previous docker container running, it might be exposed to the same port that we will use now. A good practice is to use the `docker ps` command to lists all active Docker containers along with their respective port mappings. You can stop the previous docker containers in case you don't need it.*\n",
    "\n",
    "2. Now run in the same terminal:\n",
    "\n",
    "```bash \n",
    "docker run -it --rm \\\n",
    "    -p 8080:8080 \\\n",
    "    -e PREDICTIONS_STREAM_NAME=\"ride_predictions\" \\\n",
    "    -e RUN_ID=\"e1efc53e9bd149078b0c12aeaa6365df\" \\\n",
    "    -e TEST_RUN=\"True\" \\\n",
    "    -e AWS_DEFAULT_REGION=\"eu-west-1\" \\\n",
    "    stream-model-duration:v2\n",
    "```\n",
    "\n",
    "3. Then open up a new terminal with the `code` directory (`cd mlops-zoomcamp/06-best-practices/code`) and activate the `code` evnrionment. Run `python ./integraton-test/test_docker.py`. You should see this:\n",
    "\n",
    "<img src=\"notes-images/test_docker output.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "### Running the unit tests\n",
    "\n",
    "In the code folder with the `code` environment activated, run `pytest tests/` and you should get:\n",
    "\n",
    "``` \n",
    "================================================== test session starts ==================================================\n",
    "platform darwin -- Python 3.9.6, pytest-7.1.2, pluggy-1.0.0\n",
    "rootdir: /Users/aasth/Desktop/Data analytics/MLOps/datatalks-zoomcamp/mlops-zoomcamp/06-best-practices/code\n",
    "collected 4 items                                                                                                       \n",
    "\n",
    "tests/model_test.py ....                                                                                          [100%]\n",
    "\n",
    "=================================================== 4 passed in 1.06s ===================================================\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6.2 : Integration tests with docker-compose\n",
    "\n",
    "### Integration tests:\n",
    "\n",
    "Unit tests just test partial of the code, we still need to test the whole code and we will do that using integration test. We will convert our `test_docker.py` file into an intergration test.\n",
    "\n",
    "We can do that by adding assertions and the `DeepDiff` library. The `test_docker.py` file returns a dictionary and we use deepdiff to see the difference between the expected dictionary and the returned dictionary.\n",
    "\n",
    "### Load the model first from local env and remove the dependency in S3\n",
    "\n",
    "We were loading our model from S3 in the [model.py](./code/model.py) file. We can remove that dependency by adding `get_model_location() ` function to the model file. It will check if the user has specified a local path where the model is downnloaded and load it from that path. If the user doesn't specify a path, then it will load the model from S3.\n",
    "\n",
    "Then in the `code` directory with the `code` environment activated run:\n",
    "\n",
    "```bash \n",
    "docker run -it --rm \\\n",
    "    -p 8080:8080 \\\n",
    "    -e PREDICTIONS_STREAM_NAME=\"ride_predictions\" \\\n",
    "    -e RUN_ID=\"Test123\" \\\n",
    "    -e MODEL_LOCATION=\"/app/model\" \\\n",
    "    -e TEST_RUN=\"True\" \\\n",
    "    -e AWS_DEFAULT_REGION=\"eu-west-1\" \\\n",
    "    -v \"$(pwd)/model:/app/model\" \\\n",
    "    stream-model-duration:v2\n",
    "```\n",
    "\n",
    "\n",
    "Then in another terminal (in the `code` directory with the `code` environment activated), run `python ./integraton-test/test_docker.py` to test the model which has been downloaded in local env.\n",
    "\n",
    "### Automating tests\n",
    "\n",
    "Right now to run the latest version of the tests, we need to build the docker container first and then run the docker run command and in another terminal run the test. We can automate this:\n",
    "\n",
    "1. Create a new file named `run.sh` under integration-test and changes its permissions by running `chmod +x ./integraton-test/run.sh`. `chmod +x` on a file (your script) only means, that you'll make it executable.\n",
    "\n",
    "    #### [Run.sh file](./code/integraton-test/run.sh):\n",
    "\n",
    "    - The first line is `#!/usr/bin/env bash` which basically means that we are going to use the bash command.\n",
    "\n",
    "    - The `cd \"$(dirname \"$0\")\"` takes us to the directory of our script (the model directory)\n",
    "\n",
    "    - The \n",
    "    ```\n",
    "    LOCAL_TAG=date +\"%Y-%m-%d-%H-%M\"\n",
    "    export LOCAL_IMAGE_NAME=\"stream-model-duration:${LOCAL_TAG}\"\n",
    "    ``` \n",
    "    line maintains the build version\n",
    "\n",
    "    - The `docker build -t ${LOCAL_IMAGE_NAME} ..` builds the image\n",
    "\n",
    "    - `docker compose up -d`: start docker compose\n",
    "\n",
    "    - `sleep 1`: give the container some time to start so we make the program sleep for sometime\n",
    "\n",
    "    - `ERROR_CODE=$?`: reads the exit status of the last command executed. The error code will be 0 if the script executes successfully.\n",
    "\n",
    "    - `if [${ERROR_CODE} != 0]; then docker compose logs fi`: When you see a non-zero error code then, print the docker logs\n",
    "\n",
    "    - `docker compose down`: stops containers and removes containers\n",
    "\n",
    "\n",
    "2. Create docker-compose.yaml based on the format of [Compose file reference](https://docs.docker.com/compose/compose-file/06-networks/)\n",
    "\n",
    "3. Open a terminal in a non-virtual env, run ./run.sh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6.3 : Testing cloud services with LocalStack\n",
    "\n",
    "We wrote unit tests to test our function and intergation test to test our service but we didn't test Kinesis. We need to test the Kinesis connection or the function that puts the responses to the Kinesis stream with LocalStack. LocalStack helps us to develop and test AWS applications locally.\n",
    "\n",
    "1. Add the kinesis service to the [docker-compose.yaml](./code/integraton-test/docker-compose.yaml) file.\n",
    "\n",
    "2. First `cd integraton-test` and then run `export LOCAL_IMAGE_NAME=123` and then `export PREDICTIONS_STREAM_NAME=ride_predictions` and finally `docker-compose up kinesis` in the `code` environment. \n",
    "\n",
    "3. We will create a stream locally using localstack. In another terminal, in the `code` directory and in the `code` envrionment,run:\n",
    "```bash\n",
    "aws --endpoint-url=http://localhost:4566 \\\n",
    "    kinesis create-stream \\\n",
    "    --stream-name ride_preditions \\\n",
    "    --shard-count 1\n",
    "```\n",
    "\n",
    "4. We add the `create_kinesis_client()` function in our [model.py file](./code/model.py) where we use a similar logic that we used when we removed the dependency from S3. We check if the local path to kinesis is set, then we can use that path otherwise we actually call the AWS Kinesis client.\n",
    "\n",
    "5. Stop the `docker-compose up kinesis` (by pressing control+C) and then in a new terminal in a **non-virtual envrionment**, run `cd mlops-zoomcamp/06-best-practices/code/integraton-test` and then run `./run.sh`:\n",
    "\n",
    "<img src=\"notes-images/run bash output.png\" width=\"700\"/>\n",
    "\n",
    "6. We will now check the content in the stream. Get the shard ID present in the stream by `aws --endpoint-url=http://localhost:4566 kinesis list-shards --stream-name ride_preditions`. It will return the shard ID as \"shardId-000000000000\". Use this to set the SHARD variable.\n",
    "\n",
    "```bash\n",
    "export SHARD=\"shardId-000000000000\"\n",
    "export PREDICTIONS_STREAM_NAME=ride_predictions \n",
    "aws  --endpoint-url=http://localhost:4566 \\\n",
    "    kinesis     get-shard-iterator \\\n",
    "    --shard-id ${SHARD} \\\n",
    "    --shard-iterator-type TRIM_HORIZON \\\n",
    "    --stream-name ${PREDICTIONS_STREAM_NAME} \\\n",
    "    --query 'ShardIterator'\n",
    "```\n",
    "\n",
    "_Note: Step 6 didn't work for me. It says `An error occurred (ResourceNotFoundException) when calling the GetShardIterator operation: Stream arn arn:aws:kinesis:us-east-1:000000000000:stream/ride_predictions not found`__\n",
    "\n",
    "7. Create the [test_kinesis.py file](./code/integraton-test/test_kinesis.py)\n",
    "\n",
    "8. Edit the logic in run.sh to incorporate the test_kinesis.py as well (similar logic to the test_docker.py file that we used in run.sh)\n",
    "\n",
    "9. Stop any docker containers that are running. In a new terminal in a **non-virtual envrionment**, run `cd mlops-zoomcamp/06-best-practices/code/integraton-test` and then run `./run.sh`. You can see that the kinesis service has finished running:\n",
    "\n",
    "<img src=\"notes-images/run bash kinesis output.png\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6.4: Code quality - linting and formatting\n",
    "\n",
    "In Python, it is recommended to follow the PEP8 guidelines. This ensures clean, standard code formats and helps code readability as well as minimizing diffs. To do this automatically, we use Formatters\n",
    "\n",
    "In addition to following style guides, we also want our code to be free of bad practices and deprecated language features.\n",
    "\n",
    "### Linting:\n",
    "\n",
    "We may want styled code, however, conforming to the PEP8 style manually may be cumbersome. So we can use Linters instead. Linters are pieces of software that make sure the code conforms to a certain style with minimal hinderence to the developer. For Python, a common linter is pylint. Pylint ensures that our code follows the PEP8 style guide and follows good coding practices.\n",
    "\n",
    "#### Installing pylint:\n",
    "\n",
    "1. Go to the correct directory by: `cd mlops-zoomcamp/06-best-practices/code`\n",
    "2. Activate any existing conda virtual evironment: `. /opt/homebrew/anaconda3/bin/activate && conda activate /opt/homebrew/anaconda3/envs/mlops-zoomcamp-venv;`\n",
    "3. Install pylint and create a new virtual envrinment: `pipenv install --dev pylint`. We use the dev argument cause we want pytest only in the dev environment and not in the production environment.\n",
    "4. Activate it by `pipenv shell`\n",
    "\n",
    "You can check the code quality of a file with `pylint file_name.py`, or all the files under the current folder with `pylint **/*.py`.\n",
    "\n",
    "It might be cumbersome to go through the output of the pylint for each file. You can also enabe the linter in VSCode so that the linter will warn you about good coding practice while you are coding. To do this, Hit `Cmd+Shift+P`->`Python: Select Linter`->`pylint`. Now when you open any python file, you'll see that the linter will underline lines where there is a code quality issue.\n",
    "\n",
    "#### To ignore certain errors raised by pylint:\n",
    "\n",
    "For example, let's say you want to ignore the `missing-function-docstring` error that is raised by pylint. You can do so by creating a `pyproject.toml` file in the directory where you will run pylint and include the following in the `pyproject.toml` file:\n",
    "\n",
    "`pyproject.toml` file:\n",
    "\n",
    "```toml  \n",
    "[tool.pylint.messages_control]\n",
    "\n",
    "disable = [\n",
    "    \"missing-function-docstring\"]\n",
    "```\n",
    "\n",
    "### Formatting & Imports:\n",
    "\n",
    "For formatting the code (like for example, removing trailing whitespaces), you can use a tool called `black`. To fix formatting import statements, use the tool called `isort`.\n",
    "\n",
    "Use `pipenv install --dev black isort` to install both the tools. (Follow the same steps as we did for installing pylint). \n",
    "\n",
    "It's good practice to commit your code before running the black and isort library on any of the files because these libraries will change the contents of your files. So if you commit your files before running black&isort, you can roll back to the previous version of the file easily.\n",
    "\n",
    "You can use `black --diff .` to see what changes Black would’ve made to the files but Black won't actually make any changes to the files. If you are happy with its suggested changes and want to apply it to the files, you can use `black .`. If you want Black to ignore certain changes, you can modify the `pyproject.toml` file like we did for pylint.\n",
    "\n",
    "Similarly, you can run isort by `isort --diff .` to see what changes isort would have made to files. To actually apply these changes, do `isort .`. If you want isort to ignore certain changes, you can modify the `pyproject.toml` file like we did for pylint.\n",
    "\n",
    "We can configure to run pytests,pylint,black and isort automatically before committing or using CI/CD. We will look at that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6.5 : Git pre-commit hooks\n",
    "\n",
    "Git has a way to fire off custom scripts when certain important actions occur using something called git hooks. An example of a git hook is a git post-commit hook that runs after the commit process is completed.\n",
    "\n",
    "We are going to look into the pre-commit hooks. These run first, before you even type in a commit message.\n",
    "\n",
    "#### Installing pre-commit hooks:\n",
    "\n",
    "1. Go to the correct directory by: `cd mlops-zoomcamp/06-best-practices/code`\n",
    "2. Activate any existing conda virtual evironment: `. /opt/homebrew/anaconda3/bin/activate && conda activate /opt/homebrew/anaconda3/envs/mlops-zoomcamp-venv;`\n",
    "3. Install pylint and create a new virtual envrinment: `pipenv install --dev pre-commit`. We use the dev argument cause we want pytest only in the dev environment and not in the production environment.\n",
    "4. Activate it by `pipenv shell`\n",
    "\n",
    "Let's pretend that our existing `code` directory is a new repo that we just cloned from github. In the `code` directory, do `git init` to initialize an empty git repository in the code folder. You'll see a `.git` folder being created after you execute the `git init` command. Type `ls -a` in the command line to verify that the .git folder was created.\n",
    "\n",
    "Then we need to create a `.pre-commit-config.yaml` file before running the `pre-commit` command.\n",
    "\n",
    "Type `pre-commit sample-config` (in the `code` directory with the `code` environment activated) to see see a sample content for the `.pre-commit-config.yaml` file. You can copy this content and then create a `.pre-commit-config.yaml` file and paste the contents in this file.\n",
    "\n",
    "We can add more hooks to this file by googling \"pylint pre-commit\" to see the pre-commit config file for pylint. Usually online repos will provide a config like:\n",
    "\n",
    "```yaml\n",
    "-   repo: https://github.com/pycqa/isort\n",
    "        rev: 5.10.1\n",
    "        hooks:\n",
    "        - id: isort\n",
    "          name: isort (python)\n",
    "```\n",
    "\n",
    "Run `pre-commit install` in the command line. This creates a pre-commit folder in the `.git` folder. The`.git` folder isn't committed to git. The `.git` folder is a local folder that is created which means whenever you clone a repo, you need to run `pre-commit install` first to create the pre-commit folder.\n",
    "\n",
    "Now, proceed to do `git add .` and `git commit -m \"some message\"` and you will see that some hooks may say \"Failed\" but it will have a \"files were modified by this hook\" message which means the hook modified the file to make it pass. So the next time you do `git add .` and `git commit`, you'll notice that same hook will now Pass. In the below screenshot, the \"End Of Files\" hook Failed earlier but in my second commit, it passed as the file was modified by the hook:\n",
    "\n",
    "<img src=\"notes-images/git pre-commit hook message.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6.6 : Makefiles and make\n",
    "\n",
    "*Note: Mac already installed make*\n",
    "\n",
    "Make is a tool for automating various steps for production.\n",
    "\n",
    "`make` looks for the `Makefile`, which contains the steps needed to automate the build.\n",
    "\n",
    "`make` makes use of aliases, like:\n",
    "```makefile\n",
    "run:\n",
    "    echo 123\n",
    "```\n",
    "here `run` is an alias. Now when `make run` is executed, `echo 123` is executed as a result.\n",
    "\n",
    "We can also make aliases depend on other aliases\n",
    "```makefile\n",
    "test: \n",
    "    echo test\n",
    "run: test\n",
    "    echo 123\n",
    "```\n",
    "here `run` depends on `test`, and when `make run` is executed, both `echo test` and `echo 123` are executed in that order.\n",
    "\n",
    "In our case, we want to run many things before running the program or commiting or deploying to AWS Lambda/GCP Functions/.... To do so we can make use of `Makefile`. We want to run:\n",
    "\n",
    "1. Tests (Unit tests and integration tests): using `pytest`\n",
    "2. Quality checks: `pylint`, `black`, `isort`\n",
    "\n",
    "An example on how `Makefile` can be used in our case:\n",
    "```makefile\n",
    "LOCAL_TAG:=$(shell date +\"%Y-%m-%d-%H-%M\")\n",
    "LOCAL_IMAGE_NAME:=stream-model-duration:${LOCAL_TAG}\n",
    "\n",
    "test:\n",
    "\tpytest tests/\n",
    "\n",
    "quality_checks:\n",
    "\tisort .\n",
    "\tblack .\n",
    "\tpylint --recursive=y .\n",
    "\n",
    "build: quality_checks test\n",
    "\tdocker build -t ${LOCAL_IMAGE_NAME} .\n",
    "\n",
    "integration_test: build\n",
    "\tLOCAL_IMAGE_NAME=${LOCAL_IMAGE_NAME} bash integraton-test/run.sh\n",
    "\n",
    "publish: build integration_test\n",
    "\tLOCAL_IMAGE_NAME=${LOCAL_IMAGE_NAME} bash scripts/publish.sh\n",
    "\n",
    "setup:\n",
    "\tpipenv install --dev\n",
    "\tpre-commit install\n",
    "```\n",
    "\n",
    "To run any command, you can type `make test` or `make quality_checks`, etc:\n",
    "\n",
    "<img src=\"notes-images/make file.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6b.5: CI/CD (WIP)\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "CI/CD is an import DevOps practice which is helpful for shortening the delivery of our software applications.\n",
    "\n",
    "- CI: This involves developing, testing, and packaging code in a structured manner.\n",
    "- CD: This is responsible for delivering the integrated code to various dependent applications.\n",
    "\n",
    "For our use case, we will be implementing a complete CI/CD pipeline using GitHub Actions. This pipeline will automatically trigger jobs to build, test, and deploy our service for every new commit or code change to our repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-JCzC6QQn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
